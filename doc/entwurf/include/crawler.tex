\section{Aufbau}

\subsection{Paketdiagramm}
In \cref{fig:crawler_package} ist die Grobstruktur des Crawlers als Paketdiagramm zu sehen.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth,height=0.4\textheight,keepaspectratio=true]{dia/crawler_package}
	\caption{Paketdiagramm des Crawlers}
	\label{fig:crawler_package}
\end{figure}

\begin{description}
\item[main] Dieses Paket beinhaltet die wesentliche Programmlogik des Crawlers.
\item[mysql] In diesem Paket sind Klassen, welche Methoden bereitstellen um auf eine MySQL-Datenbank zuzugreifen. Außerdem ist noch ein Paket mit Klassen zur Verwaltung von Twitter-Daten integriert.
\item[locate] Das locate-Paket dient zur Lokalisierung von Twitter-Accounts und Retweets mittels Webdiensten.
\item[java 1.8] Standard java 1.8 Bibliothek.
\item[twitter4j] twitter4j Bibliothek.
\end{description}

\subsection{Klassendiagramm}
Zum Sammeln der Daten von Twitter verwenden wir einen Crawler, welcher über die Twitter-Streaming-API Daten sammelt. Dazu ist es nötig diese Daten zu empfangen, dann zu puffern und schlussendlich in die Datenbank zu schreiben. Allerdings müssen die Daten noch gefiltert werden, da wir nur Daten von verifizierten Twitter-Accounts (und manuell Hinzugefügten) speichern. Sind die Daten gefiltert, werden sie vom Crawler über einen Webdienst lokalisiert. Das heißt, dass jedem Account, beziehungsweise jedem Retweet ein Ort zugeordnet wird. Ist dies erfolgt so werden die Daten in die Datenbank geschrieben.
\\ In \cref{fig:uml_crawler} ist der Aufbau des Crawlers anhand eines UML-Klassendiagramms mit zugehöriger Beschreibung spezifiziert.\\

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth,height=\textheight,keepaspectratio=true]{dia/uml_crawler}
	\caption{UML-Klassendiagramm des Crawlers}
	%\includegraphics[width=0.9\textheight,height=\textwidth,keepaspectratio=true,angle=-90]{dia/uml_crawler}
	\label{fig:uml_crawler}
\end{figure}

\begin{description}
\item[CrawlerMain] Diese Klasse enthält die main-Methode zum Aufrufen des Programms. Sie überprüft die Eingabe für die Datenbankverbindung und startet einen Controller. Danach steht sie dem Benutzer zur Überwachung des Programms über die Konsole zur Verfügung.
\item[RunnableListener] Interface, welches Runnable erweitert und zusätzlich noch eine exit-Methode fordert um Threads zu beenden.
\item[Controller] Diese Klasse koordiniert alle Aktionen die nötig sind um Daten bei Twitter abzuholen und in die Datenbank zu schreiben. Dazu startet sie einen StreamListener, ein AccountUpdate und mehrere StatusProcessors jeweils als Thread. Außerdem kontrolliert sie den Puffer und sorgt für ein sauberes Beenden des Programms indem alle Verbindungen ordnungsgemäß geschlossen und die Threads beendet werden.
\item[StreamListener] Stellt eine Verbindung zur Twitter-Streaming-API her und initialisiert einen MyStatusListener.
\item[AccountUpdate] Diese Klasse stellt eine Methode zur Verfügung um in der Datenbank periodisch nach Accounts zu suchen, welche manuell hinzugefügt wurden, nicht verifiziert sind, aber auch getracked werden sollen.
\item[StatusProcessor] Diese Klasse stellt die Funktionalität zur Filterung der Daten von Twitter zur Verfügung, welche sie aus dem Puffer nimmt. Außerdem bietet sie die Möglichkeit diese Daten zu vervollständigen und in die Datenbank zu schreiben.
\item[MyStatusListener] Diese Klasse nimmt die Daten von Twitter entgegen und schreibt diese in einen Puffer.
\item[MyConnectionLifeCycleListener] Diese Klasse verwaltet die Verbindung zu Twitter bezüglich Verbindungsabbrüchen und stellt bei Bedarf wieder eine Verbindung her.
\item[Locator] Der Locator lokalisiert Accounts und Retweets mithilfe mehrerer Webdienste.
\item[DBIcrawler] siehe \cref{sec:datenbankzugriff}
\end{description}

\section{Start des Crawlers}
Beim Starten des Crawlers werden sämtliche notwendigen Komponenten der Reihe nach gestartet. Dadurch wird garantiert, dass jede Komponente eine Umgebung vorfindet in der sie lauffähig ist und alle Ressourcen bereits zur Verfügung stehen. In \cref{fig:crawler_start} ist der Start des Crawlers beispielhaft mit 2 StatusProcessors dargestellt.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth,height=\textheight,keepaspectratio=true]{dia/crawler_start_sequence}
	%\includegraphics[width=0.9\textheight,height=0.9\textwidth,keepaspectratio=true,angle=-90]{dia/crawler_start_sequence}
	\caption{Sequenzdiagramm zum Start des Crawlers}
	\label{fig:crawler_start}
\end{figure}

Um den Crawler zu starten, werden ihm die Zugriffsdaten auf die Datenbank übergeben und die Anzahl der Threads die später Daten verarbeiten (Hardware abhängig, wird vom Benutzer festgelegt).
Die main-Methode der Main-Klasse instantiiert daraufhin ein Controller Objekt, welches ab dann sämtliche Steuerung übernimmt. Dieses Controller Objekt wird dann in einem Thread ausgeführt. Die Main-Klasse ist nun nur noch dafür zuständig Benutzereingaben entgegenzunehmen und diese weiter zu delegieren.
Sobald der Controller gestartet wurde instantiiert er die StatusProcessor Objekte, welche später sämtliche Daten verarbeiten müssen. Außerdem werden noch ein AccountUpdate- und ein StreamListener-Objekt instantiiert. Ersteres dient dazu eine Liste nicht verifizierter Accounts, die dennoch getracked werden sollen, aus der Datenbank aktuell zu halten. Zweiteres ist dafür zuständig eine Verbindung zur Twitter-Streaming-API einzurichten. Alle diese Objekte werden dann vom Controller als eigenständige Threads ausgeführt.
Daraufhin beginnt der StreamListener eine Verbindung zur Twitter-Streaming-API herzustellen (siehe \cref{fig:initialize_stream}).\\
Nun ist der Crawler im aktiven Zustand und empfängt Daten von Twitter, welche dann gefiltert, vervollständigt und in der Datenbank abgelegt werden.\\
Wird der Crawler vom Benutzer beendet, so sendet der Controller jedem Objekt, welches in einem Thread läuft ein exit-Signal. Daraufhin beenden die Objekte ihre Tätigkeit und der jeweilige Thread kehrt zum Controller zurück, sodass dieser dann das gesamte Programm beenden kann.
\\

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth,height=\textheight,keepaspectratio=true]{dia/crawler_initialize_stream}
	%\includegraphics[width=\textheight,height=\textwidth,keepaspectratio=true,angle=-90]{dia/crawler_initialize_stream}
	\caption{Sequenzdiagramm zur Initialisierung des TwitterStreams}
	\label{fig:initialize_stream}
\end{figure}

Um eine Verbindung zur Twitter-Streaming-API herzustellen, wird zuerst ein ConnectionLifeCycleListener erstellt um auf Verbindungsabbrüche reagieren zu können (indem z.B. die Verbindung neu hergestellt wird). Danach wird noch ein Filter erstellt mit dem der Twitterstream durchsucht wird und ein MyStatusListener um über eingehende Daten benachrichtigt zu werden.
Anschließend wird ein TwitterStream-Objekt aus der twitter4j-Bibliothek geholt (als Singleton). Auf diesem werden dann die Methoden zur Initialisierung aufgerufen und somit die Listener und der Filter gesetzt.\\
Zum Schließen des Streams wird auf dem TwitterStrem-Objekt die shutdown-Methode aufgerufen.

\section{Verarbeitung der Daten von Twitter}
Um zu verdeutlichen wie die Daten von Twitter innerhalb des Crawlers verarbeitet werden, ist in \cref{fig:crawler_process} der Datenfluss durch den Crawler exemplarisch dargestellt. Dabei werden die Daten von Twitter abgeholt, gepuffert, dann gefiltert und schlussendlich in die Datenbank geschrieben.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth,height=\textheight,keepaspectratio=true]{dia/crawler_process_sequence}
	%\includegraphics[width=\textheight,height=\textwidth,keepaspectratio=true,angle=-90]{dia/crawler_process_sequence}
	\caption{Sequenzdiagramm der Verarbeitung der Daten von Twitter}
	\label{fig:crawler_process}
\end{figure}

Zuerst werden die Daten, welche im MyStatusListener auflaufen in eine (threadsichere) Warteschlange geschoben.
Hat dann ein StatusProcessor freie Kapazitäten so entnimmt er das erste Element der Warteschlange. Dieses Status-Objekt enthält alle relevanten Informationen um zu entscheiden, ob das Objekt interessant ist oder nicht.
Interessante Status-Objekte enthalten verifizierte Twitter-Accounts oder Retweets auf Tweets von verifizierten Accounts. Ist ein verifizierter Account in einem Status-Objekt gefunden, so wird der Account (hier: user) für die Datenbank freigegeben. Das heißt das DBcrawler-Objekt übergibt
dem Locator den Account zur Lokalisierung, falls dieser noch nicht in der Datenbank steht. Dieser versucht dann anhand eines Orts-Strings den Account einem Land zuzuordnen. Ist dies geschehen so wird der Account in die Datenbank geschrieben. 
Bei Status-Objekten die Retweets enthalten wird genau nach dem selben Schema vorgegangen, allerdings übernimmt hier das StatusProcessor-Objekt die Aufgabe den Retweet zu lokalisieren.
Ist ein solches Status-Objekt verarbeitet, so beginnt der ganze Verarbeitungsprozess wieder von vorne.
